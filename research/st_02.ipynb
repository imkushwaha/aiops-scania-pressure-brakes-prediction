{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder,StandardScaler\n",
    "\n",
    "from pressure_brake_prediction.exception import CustomException\n",
    "from pressure_brake_prediction.logger import logging\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class DataIngestionConfig:\n",
    "    root_dir: Path\n",
    "    source_data_file: Path\n",
    "    local_data_file: Path\n",
    "    unzip_dir: Path\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class DataTransformationConfig:\n",
    "    root_dir: Path\n",
    "    raw_train_data: Path\n",
    "    raw_test_data:  Path\n",
    "    train_data: Path\n",
    "    test_data: Path\n",
    "    target_column:  str\n",
    "    preprocessor_obj_file_path: Path\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pressure_brake_prediction.constants import *\n",
    "from pressure_brake_prediction.utils import read_yaml, create_directories, save_bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConfigurationManager:\n",
    "    def __init__(\n",
    "            self, \n",
    "            config_filepath = CONFIG_FILE_PATH, \n",
    "            params_filepath = PARAM_FILE_PATH):\n",
    "        self.config = read_yaml(config_filepath)\n",
    "        self.params = read_yaml(params_filepath)\n",
    "\n",
    "        create_directories([self.config.artifacts_root])\n",
    "\n",
    "    def get_data_ingestion_config(self) -> DataIngestionConfig:\n",
    "\n",
    "        config = self.config.data_ingestion\n",
    "\n",
    "        create_directories([config.root_dir])\n",
    "\n",
    "        data_ingestion_config = DataIngestionConfig(\n",
    "            root_dir=config.root_dir,\n",
    "            source_data_file=config.source_data_file,\n",
    "            local_data_file=config.local_data_file, \n",
    "            unzip_dir=config.unzip_dir\n",
    "        )\n",
    "\n",
    "        return data_ingestion_config\n",
    "    \n",
    "    def get_data_transformation_config(self) -> DataTransformationConfig:\n",
    "        config = self.config.data_transformation\n",
    "\n",
    "        create_directories([config.root_dir])\n",
    "\n",
    "        data_transformation_config = DataTransformationConfig(\n",
    "            root_dir=config.root_dir, \n",
    "            raw_train_data = config.raw_train_data, \n",
    "            raw_test_data = config.raw_test_data,\n",
    "            train_data = config.train_data, \n",
    "            test_data = config.train_data,\n",
    "            preprocessor_obj_file_path = config.preprocessor_obj_file_path,\n",
    "            target_column = config.target_column\n",
    "        )\n",
    "\n",
    "        return data_transformation_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import sys\n",
    "import tqdm\n",
    "from pressure_brake_prediction.exception import CustomException\n",
    "import joblib\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pressure_brake_prediction.logger import logging\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "class DataTransformation:\n",
    "    def __init__(self, config: DataTransformationConfig):\n",
    "        self.config = config\n",
    "\n",
    "    def get_data_transformer_object(self):\n",
    "\n",
    "        \"This function is reponsible for data transformation.\"\n",
    "\n",
    "        try:\n",
    "        \n",
    "            num_pipeline= Pipeline(\n",
    "                    steps=[\n",
    "                    (\"imputer\",SimpleImputer(strategy=\"mean\")),\n",
    "                    (\"scaler\",StandardScaler()),\n",
    "                    (\"pca\", PCA(n_components=65, random_state=42))\n",
    "                    ]\n",
    "            )\n",
    "\n",
    "            return num_pipeline\n",
    "        except Exception as e:\n",
    "            raise CustomException(e,sys)\n",
    "    \n",
    "    def initiate_transformer(self):\n",
    "\n",
    "        try:\n",
    "\n",
    "            train_data = pd.read_csv(self.config.raw_train_data, na_values=\"na\")\n",
    "            test_data = pd.read_csv(self.config.raw_test_data, na_values = \"na\")\n",
    "\n",
    "            logging.info(\"Read train and test data completed\")\n",
    "\n",
    "            logging.info(\"Obtaining preprocessing object\")\n",
    "\n",
    "            preprocessing_obj=self.get_data_transformer_object()\n",
    "\n",
    "            X_train_data = train_data.drop(columns=[self.config.target_column], axis=0)\n",
    "            y_train_data = train_data[self.config.target_column]\n",
    "\n",
    "            X_test_data = test_data.drop(columns=[self.config.target_column], axis=0)\n",
    "            y_test_data = test_data[self.config.target_column]\n",
    "\n",
    "            logging.info(\"Checking for data imbalance in Train data.\")\n",
    "\n",
    "            logging.info(str(y_train_data.value_counts()))\n",
    "\n",
    "            # data is highly balance doing oversampling for getting a balanced data\n",
    "\n",
    "            oversample_obj = RandomOverSampler(random_state=42)\n",
    "\n",
    "            oversampled_X_train_data, oversampled_y_train_data = oversample_obj.fit_resample(X_train_data, y_train_data)\n",
    "\n",
    "            # data is oversampled now check if it is balanced.\n",
    "            logging.info(f\"Data after transformation {oversampled_y_train_data.value_counts()}\")\n",
    "\n",
    "            # initiating the pipeline for data transformatoin\n",
    "            logging.info(\n",
    "                    f\"Applying preprocessing object on training dataframe and testing dataframe.\"\n",
    "                )\n",
    "\n",
    "            processed_train_data = preprocessing_obj.fit_transform(oversampled_X_train_data)\n",
    "            processed_test_data = preprocessing_obj.transform(X_test_data)\n",
    "\n",
    "\n",
    "            train_arr = np.c_[\n",
    "                    processed_train_data, np.array(oversampled_y_train_data)]\n",
    "            test_arr = np.c_[\n",
    "                    processed_test_data, np.array(y_test_data)]\n",
    "            \n",
    "            logging.info(\"Saved preprocessing object.\")\n",
    "            save_bin(\n",
    "\n",
    "                    path=self.config.preprocessor_obj_file_path,\n",
    "                    data=preprocessing_obj\n",
    "\n",
    "                )\n",
    "            \n",
    "            save_bin(\n",
    "\n",
    "                    path=self.config.train_data,\n",
    "                    data=train_arr\n",
    "\n",
    "                )\n",
    "            \n",
    "            save_bin(\n",
    "\n",
    "                    path=self.config.test_data,\n",
    "                    data=test_arr\n",
    "\n",
    "                )\n",
    "\n",
    "            return (train_arr,\n",
    "                    test_arr,\n",
    "                    self.config.preprocessor_obj_file_path)\n",
    "        except Exception as e:\n",
    "            raise CustomException(e,sys)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-04-09 12:36:35,889: INFO: common: yaml file: config/config.yaml loaded successfully]\n",
      "[2023-04-09 12:36:35,903: INFO: common: yaml file: params.yaml loaded successfully]\n",
      "[2023-04-09 12:36:35,906: INFO: common: created directory at: artifacts]\n",
      "[2023-04-09 12:36:35,909: INFO: common: created directory at: artifacts/data_transformation]\n",
      "[2023-04-09 12:36:38,039: INFO: 2379095977: Read train and test data completed]\n",
      "[2023-04-09 12:36:38,040: INFO: 2379095977: Obtaining preprocessing object]\n",
      "[2023-04-09 12:36:38,159: INFO: 2379095977: Checking for data imbalance in Train data.]\n",
      "[2023-04-09 12:36:38,170: INFO: 2379095977: neg    59000\n",
      "pos     1000\n",
      "Name: class, dtype: int64]\n",
      "[2023-04-09 12:36:40,178: INFO: 2379095977: Data after transformation neg    59000\n",
      "pos    59000\n",
      "Name: class, dtype: int64]\n",
      "[2023-04-09 12:36:40,179: INFO: 2379095977: Applying preprocessing object on training dataframe and testing dataframe.]\n",
      "[2023-04-09 12:36:53,635: INFO: 2379095977: Saved preprocessing object.]\n",
      "[2023-04-09 12:36:53,671: INFO: common: binary file saved at: artifacts/data_transformation/processor.joblib]\n",
      "[2023-04-09 12:36:54,172: INFO: common: binary file saved at: artifacts/data_transformation/train.joblib]\n",
      "[2023-04-09 12:36:54,242: INFO: common: binary file saved at: artifacts/data_transformation/train.joblib]\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    config = ConfigurationManager()\n",
    "    data_transformation_config = config.get_data_transformation_config()\n",
    "    data_transformation = DataTransformation(data_transformation_config)\n",
    "    data4 = data_transformation.initiate_transformer()\n",
    "except Exception as e:\n",
    "    raise CustomException(e, sys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data4[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
